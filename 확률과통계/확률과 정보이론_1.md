# 확률과 정보이론 개념 종합 정리

## 1. 확률(Probability)

- **정의**: 특정 사건이 일어날 가능성을 0과 1 사이의 수치로 표현
- **수학적 표현**: P(A) = 사건 A가 일어날 경우의 수 / 전체 경우의 수
- **특성**:
  - 어떠한 가측공간에서 사건이 일어날 확률은 0보다 크거나 같고 1보다 작거나 같다 (0 ≤ P(A) ≤ 1)
  - 모든 확률의 합은 1이다 (∑P(A) = 1)
- **예시**: 공정한 주사위에서 3이 나올 확률 = 1/6

## 2. 기대치(Expected Value)

- **정의**: 확률 변수의 가능한 모든 값에 각 값의 확률을 곱하여 합산한 것, 확률 변수의 평균
- **수학적 표현**: 
  - 이산: E(X) = ∑ x_i × P(X = x_i) 
  - 연속: E(X) = ∫ x × f(x) dx
- **특징**: 확률적 실험을 무한히 반복했을 때 관측되는 평균값
- **예시**: 주사위의 기대값 = 1×(1/6) + 2×(1/6) + ... + 6×(1/6) = 3.5

## 3. 정보량(Information Content)

- **정의**: 특정 사건이 제공하는 정보의 양, 불확실성을 제거하는데 필요한 정보량
- **수학적 표현**: I(x) = -log₂(P(x))
- **특징**: 
  - 확률이 낮은 사건일수록 정보량이 큼 (희귀한 사건은 더 많은 정보 제공)
  - 정보량이 클수록 불확실성이 커짐(일어날 확률 P(x)가 작기 때문)
- **단위**: 비트(bit)

## 4. 엔트로피(Entropy)

- **정의**: 확률 분포의 불확실성 또는 정보량의 기대값
- **수학적 표현**: H(X) = -∑ P(x) log₂P(x)
- **특징**: 
  - 확률 분포가 균등할수록 엔트로피가 최대
  - 결과가 확정적일수록 엔트로피는 0에 가까움
  - 공정한 주사위는 모든 눈의 확률이 동일하여 예측하기 어려우므로 엔트로피가 높음
  - 불공정한 주사위는 특정 눈이 나올 확률이 높아 예측이 상대적으로 쉬우므로 엔트로피가 낮음
  - 모든 확률이 균등할 때 각 사건의 정보량과 전체 분포의 엔트로피가 같아짐(log₂(n))

## 5. 확률질량함수(PMF, Probability Mass Function)

- **정의**: 확률변수 X가 이산확률변수일 때 각 값에 대한 확률을 정의하는 함수
- **수학적 표현**: P(X = x) = f(x)
- **특징**: 모든 가능한 값의 확률 합은 1
- **예시**: 주사위의 PMF는 f(x) = 1/6, x ∈ {1,2,3,4,5,6}
- **대표적 분포**:
  - **베르누이 분포**: 성공/실패만 있는 단일 시행 (동전 던지기)
    - PMF: P(X=1) = p, P(X=0) = 1-p
    - 엔트로피: H(X) = -[p·log₂(p) + (1-p)·log₂(1-p)]
  - **이항분포**: n번의 독립 베르누이 시행에서 성공 횟수

## 6. 확률밀도함수(PDF, Probability Density Function)

- **정의**: 확률변수 X가 연속확률변수일 때 확률 분포를 나타내는 함수
- **특징**: 
  - 구간에 대한 적분값이 확률을 의미 (P(a ≤ X ≤ b) = ∫[a,b] f(x) dx)
  - 전체 구간의 적분값은 1 (∫[-∞,∞] f(x) dx = 1)
  - PDF 자체는 확률이 아님, 특정 점에서의 값은 확률밀도를 의미
- **대표적 분포**:
  - **정규분포(가우시안 분포)**: 자연현상을 모델링하는 데 널리 사용
    - PDF: f(x) = (1/(σ√2π)) * e^(-(x-μ)²/(2σ²))
    - μ: 평균, σ: 표준편차

## 7. 특수한 분포의 엔트로피

- **균등 분포**: 모든 확률이 동일한 분포
  - n개 가능한 결과에 대한 엔트로피: log₂(n)
  - 최대 불확실성을 나타냄
- **베르누이 분포**: 
  - 최대 엔트로피: p = 0.5일 때 1비트
  - 최소 엔트로피: p = 0 또는 p = 1일 때 0비트

이러한 개념들은 정보이론, 통계학, 머신러닝, 통신 시스템 설계 등 다양한 분야에서 기초적이면서도 핵심적인 역할을 합니다. 특히 엔트로피 개념은 정보 압축, 데이터 전송 최적화, 의사결정 알고리즘 등에 광범위하게 응용됩니다.