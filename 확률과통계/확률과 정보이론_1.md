# 확률과 정보이론 관련 개념 종합 정리

## 1. 확률(Probability)

- **정의**: 특정 사건이 일어날 가능성을 0과 1 사이의 수치로 표현
- **수학적 표현**: P(A) = 사건 A가 일어날 경우의 수 / 전체 경우의 수
- **예시**: 공정한 주사위에서 3이 나올 확률 = 1/6

## 2. 기대치(Expected Value)

- **정의**: 확률 변수의 가능한 모든 값에 각 값의 확률을 곱하여 합산한 것
- **수학적 표현**: E(X) = Σ x_i × P(X = x_i) (이산), E(X) = ∫ x × f(x) dx (연속)
- **특징**: 확률 변수의 "이론적 평균", 장기적으로 실험을 반복했을 때 관측되는 평균값
- **예시**: 주사위의 기대값 = 1×(1/6) + 2×(1/6) + ... + 6×(1/6) = 3.5
- **사용자 질문**: "기대치라는게 확률 변수의 평균이라고?"
  - **답변**: 네, 기대치는 확률적 실험을 무한히 반복할 때 나타나는 평균값으로, 확률 변수의 이론적 평균입니다.

## 3. 정보량(Information Content)

- **정의**: 특정 사건이 제공하는 정보의 양을 수치화
- **수학적 표현**: I(x) = -log₂(P(x))
- **특징**: 확률이 낮은 사건일수록 정보량이 큼 (희귀한 사건은 더 많은 정보를 제공)
- **단위**: 비트(bit)

## 4. 엔트로피(Entropy)

- **정의**: 확률 분포의 불확실성 또는 정보량의 기대값
- **수학적 표현**: H(X) = -Σ P(x) log₂P(x)
- **특징**: 
  - 확률 분포가 균등할수록 엔트로피가 최대
  - 결과가 확정적일수록 엔트로피는 0에 가까움
- **사용자 질문**: "엔트로피는 정보량에 대한 기대치니까 엔트로피가 크다는건 불확실성에 대한 평균값이 크다는거네?"
  - **답변**: 네, 엔트로피는 정보량의 기대치로, 불확실성의 평균적 크기를 나타냅니다.
- **사용자 질문**: "따라서 엔트로피가 크다는건 해당 사건이 발생할 확률이 작은거고?"
  - **답변**: 아니요, 엔트로피가 크다는 것은 확률 분포가 더 균등하여 예측이 어렵다는 의미입니다. 특정 결과의 확률이 작다는 의미가 아닙니다.
- **사용자 질문**: "주사위로 예를 들면... 공정한 주사위는 엔트로피가 높고 불공정한 주사위는 엔트로피가 낮은거고?"
  - **답변**: 네, 공정한 주사위는 모든 눈의 확률이 동일하여 예측하기 어려우므로 엔트로피가 높고, 불공정한 주사위는 특정 눈이 나올 확률이 높아 예측이 상대적으로 쉬우므로 엔트로피가 낮습니다.
- **사용자 질문**: "모든 확률변수가 균등하면 정보량과 엔트로피값이 같네?"
  - **답변**: 네, 모든 확률이 균등할 때 각 사건의 정보량과 전체 분포의 엔트로피가 같아집니다(log₂(n)). 이는 최대 불확실성 상태를 의미하며, 모든 사건이 동일한 정보를 제공함을 의미합니다.

## 5. 확률질량함수(PMF, Probability Mass Function)

- **정의**: 이산확률변수에서 각 값에 대한 확률을 정의하는 함수
- **특징**: 이산확률변수에서만 사용, 모든 가능한 값의 확률 합은 1
- **예시**: 주사위의 PMF는 f(x) = 1/6, x ∈ {1,2,3,4,5,6}

## 6. 확률밀도함수(PDF, Probability Density Function)

- **정의**: 연속확률변수에서 확률 분포를 나타내는 함수
- **특징**: 구간에 대한 적분값이 확률을 의미, 전체 구간의 적분값은 1
- **PDF 자체는 확률이 아님**: 특정 점에서의 값은 확률밀도를 의미함

## 7. 특수한 분포의 엔트로피

- **베르누이 분포**: H(X) = -[p·log₂(p) + (1-p)·log₂(1-p)]
  - 최대 엔트로피: p = 0.5일 때 1비트
  - 최소 엔트로피: p = 0 또는 p = 1일 때 0비트
- **균등 분포**: 엔트로피 = log₂(n), 여기서 n은 가능한 결과의 수

이상의 개념들은 정보이론, 통계학, 머신러닝 등 다양한 분야에서 핵심적인 역할을 합니다. 특히 엔트로피 개념은 정보 압축, 통신 시스템, 의사결정 알고리즘 등에 광범위하게 응용됩니다.