# 대칭행렬(Symmetric Matrix)
**정의** : 전치 행렬이 자기 자신과 같은 정방행렬
$$A = A^T$$
## 전치 행렬(transposed matrix)
**정의** : 행과 열을 교환하여 얻는 행렬. 주대각선을 축으로 하는 반사 대칭을 가해 얻는 행렬.

- 왜 **대칭행렬**에 관심이 있는가?
> Feature들 간의 함수관계 중 가장 간단한 함수관계인 선형관계를 파악하려면 Covariance를 계산해 봐야하는데 모든 Feature간의 공분산 $Cov(Z,Z)$가 Symmetric Matrix임

---
# 고윳값(eigenvalues)
**정의** : 고유 벡터의 길이가 변하는 배수
- 고유 벡터가 얼마나 스케일링되는지를 나타냄
- 선형 변환의 그 고유 벡터에 대응하는 값($\lambda$)
- 고윳값의 성질
	- $Tr(A) = \sum \lambda$
	- $det(A) = \prod \lambda$

---
# 고유벡터(eigenvector)
**정의** : 그 선형 변환이 일어난 후에도 방향이 변하지 않는, 0이 아닌 벡터이다.
즉, $A$라는 선형 변환에 대해 $Ax = \lambda x$를 만족하는 0이 아닌 벡터 $x$가 $eigenvector$이며, 그 **각 벡터의 크기변환에 대응되는 계수**가 $eigenvalues$이다.

---
# 고윳값분해(eigendecomposition)
- 고윳값 분해의 **기본 아이디어** : 행렬 $A$가 나타내는 선형 변환이 가능한 가장 간단한 방식으로 작용하는 기저(고유벡터로 형성)를 식별하는 것.
- 즉, 각 기저 벡터를 따라 스케일링이 일어나고, 스케일링 계수는 고윳값.
- 이 단순화(대각화)는 선형 변환의 본질을 이해하는 데 뿐만 아니라 행렬과 관련된 계산을 용이하게 하는 데 매우 중요.
- n차원 행렬 A의 n개의 선형 독립인 고유벡터는 선형 변환 하에서(스케일링까지) 불변하는 벡터 공간의 방향을 정의하고, 고윳값은 이 스케일링의 크기를 정량화함.
- 쉽게 말해 복잡한 행렬 $A$를 쉽게 나눠서 표현이 가능
$$A = PDP^{-1}$$
여기서:
- $P$는 $A$의 $eigenvector$들을 열벡터로 갖는 가역행렬
- $D$는 $A$의 $eigenvalues$들을 대각 원소로 갖는 대각행렬
## 1. 고윳값 구하기
$$Ax = \lambda x$$
이를 $x$에 대해 정리하면, + 단위행렬 $I$ 와 $\lambda$(스칼라)를 곱해 $A$의 차원을 맞춤
$$(A - \lambda I)x = 0$$
비자명한 해 $x\neq 0$가 존재하기 위한 필요충분 조건은
$$\det(A - \lambda I) = 0$$
을 만족하는 것

> $\det(A)$이 존재하면 $A^{-1}$이 존재 $\rightarrow$ 자명한 해($x=0$)를 유일한 해로 가짐
> 비자명한 해(고유벡터)를 얻기 위해서는 $\det(A - \lambda I)=0$여야 함.
> 양변에 역행렬을 곱했을 때

> $det(A) = 0$ 이 가지는 의미
> 1. 역행렬 존재 X
> 2. 해가 무한히 많거나 존재하지 않음
> 3. 부피의 소멸 : 기하학적으로 determinant는 변환 후 벡터들이 이루는 공간의 부피를 나타내는데, 이 값이 0이면 부피가 0이 됨. -> 공간 "붕괴"
> 4. 차원 축소 : 행렬식이 0이면 행렬이 나타내는 선형 변환이 공간을 "압축"하여 차원을 줄임
> 5. 선형 종속 : 행렬의 행(또는 열)들이 선형 종속관계임, 행렬의 Rank가 행렬의 차원보다 작음
> 6. $det(A) = \prod \lambda$
## 2. 고유벡터 구하기
각 고유값 $\lambda$에 대응되는 고유벡터 $x_i$는
$$(A - \lambda_i I)x_i = 0$$
의 해를 구하여 찾음. 이때, 여러 고유벡터가 존재할 수 있으며, $A$가 대각화 가능하려면 $A$의 차원만큼 선형독립(linear independent)인 eigenvector가 필요
## 3. $A=PDP^{-1}$
- $A$의 고유벡터들$\begin {bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}$을 모아 $P$ 구성()
$$P = \begin{bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}$$
- $A$의 고유값들을 대각 원소로 가지는 대각행렬 $D$ 구성
$$
D = \begin{bmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{bmatrix}
$$
## 4. 대각화 검증
$A$의 고윳값과 고유벡터로 위와 같이($P, D$)가 표현된다면, 다음 관계가 성립
$$A P = P D$$
양변에 $P^{-1}$를 곱하면,
$$A = P D P^{-1}$$
---
## 정방행렬 $A$가 $A \neq A^T$일때 대각화
- 고윳값이 복소수일 수 있음
- 고유벡터들이 직교하지 않거나 충분한 선형 독립성을 보장하지 않을 수 있음.
	- 완전한 기저를 이루지 못할 수 있음 $\rightarrow$ 결함(detective)이 존재
	- 이는 전체 n차원 공간을 기저를 이루기에 충분한 n개의 선형 독립적인 고유벡터를 찾지 못한다는 뜻(일부 고유벡터들이 서로 **종속**)

## 회전행렬(rotation matrix)의 대각화
$$
R(\theta) =
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}
$$

- $\theta = 90^\circ$일때
$$R(90^\circ) =
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}$$
- $(A-\lambda I)x=0$ 꼴에서 $det(A-\lambda I)=0$ 을 만족해야 함.
$$
\det
\begin{bmatrix}
-\lambda & -1 \\
1 & -\lambda
\end{bmatrix}
$$
$$
\lambda^2 + 1 = 0 \quad \Rightarrow \quad \lambda^2 = -1 \quad \Rightarrow \quad \lambda = \pm i
$$
$\rightarrow$ $R(90^\circ)$ 행렬에 경우 고유벡터는 존재하지 않음.(실수 공간에서는 방향을 그대로 유지하는 고유벡터가 없고, 그 고유값이 허수로 나타남)

---
## 선형 변환(linear transformation)
**정의** : 선형대수학에서 선형 결합을 보존하는, 두 벡터 공간 사이의 함수
다음 **두 가지 조건**을 만족하는 변환을 선형 변환이라고 함.
- 선형 변환 $A$가 벡터 $v$에 대해 $A(v)$를 계산할 때,
$\rightarrow$ **스칼라 곱 보존**($\alpha$는 스칼라)
$$A(\alpha v) = \alpha A(v)$$
$\rightarrow$ **덧셈 보존**($v_1, v_2$는 벡터)
$$A(v_1 + v_2) = A(v_1) + A(v_2)$$
이 성립함
### **선형 변환의 의미**
1. 크기변화 및 회전변환
2. 차원변환
---
### 선형 결합(linear combination)
**정의** : 수학에서 각 항에 상수를 곱하고 결과를 더함으로써 일련의 항으로 구성된 표현식.
$x$와 $y$의 선형 결합은 $ax + by$ 형식인데 여기서 $a,b$는 상수
$ex.$ $v_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, v_2=\begin{bmatrix} 0 \\ 1 \end{bmatrix}$ 에 대해,
$$3v_1+4v_2= \begin{bmatrix} 3 \\ 4 \end{bmatrix}$$
는 $v_1$과 $v_2$의 선형 결합임.

# 대칭행렬과 데이터 분석
## 1. 고윳값(Eigenvalue)과 고유벡터(Eigenvector)
- 고윳값과 고유벡터로 행렬$A$을 분해함으로써, 데이터가 **어떤 축**으로 바라봤을 때 가장 중요한 특징(분산 등)이 드러나는지를 알 수 있음.
- **공분산 행렬** $\Sigma = \mathrm{Cov}(Z,Z)$의 고유벡터는 데이터가 분산되는 “주된 방향”을 의미하고, 고윳값은 그 방향으로의 **분산 정도**를 나타냄.

## 2. 공분산 행렬과 PCA
- **공분산 행렬 $\Sigma$** 는 대칭행렬, 직교대각화(orthogonal diagonalization)가 가능함.
- **PCA(주성분 분석)** 는 사실상 공분산 행렬 $\Sigma$를 직교대각화하는 과정과 동일함.
    - $\Sigma$를 대각화했을 때, 대각원소(고윳값)가 큰 순서대로 대응되는 고유벡터를 **주성분(principal component)** 이라고 부름.
    - 상위 몇 개의 주성분만으로 원 데이터 분산의 대부분을 설명할 수 있다면, 그 고유벡터들에 **데이터를 사영**(projection)하여 차원축소를 달성할 수 있음.

## 3. 선형회귀(Linear Regression)와 사영행렬
- 선형회귀에서 예측값 $\hat{y}$를 구하는 **행렬 P** 는 $P=Z(Z′Z)−1Z′P = Z(Z'Z)^{-1}Z'$ 로 정의되며, 이는 **대칭(symmetric)** 이면서 **멱등(idempotent)** 인 행렬임.
- **사영(projection)** 의 본질
    - 실제 관측치 y를 Z의 컬럼공간(column space)으로 **투영**하여 예측값 $y$를 얻음.
    - 사영행렬이 대칭이므로, 한 번 투영된 벡터를 다시 투영해도 변화가 없다는 멱등성$(P2=PP^2 = P)$을 가짐.
    - 이로 인해 계산과 해석이 단순화됨 (이미 투영된 벡터는 더 이상 변하지 않음).
