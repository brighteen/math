# 조건부 엔트로피(Conditional Entropy) 수식 분석

제시된 이미지에는 조건부 엔트로피의 정의와 이를 결합 엔트로피와 연결하는 관계식이 표현되어 있습니다. 이 수식들을 단계별로 살펴보겠습니다.

## 수식 설명

### 조건부 엔트로피의 정의
이미지에서 Y가 주어졌을 때 X의 조건부 엔트로피는 다음과 같이 정의됩니다:

1. 첫 번째 표현:
   $$H_{p_{X,Y}}(X|Y=y) = E_{X,Y}[log₂(1/p(X|Y))]$$
   - 이는 X의 조건부 확률분포에서의 정보량의 기댓값을 의미합니다.

2. 두 번째 표현:
   $$= E_{X,Y}[log₂(p(Y)/p(X,Y))]$$
   - 조건부 확률의 정의 p(X|Y) = p(X,Y)/p(Y)를 이용하여 변형한 형태입니다.

3. 세 번째 표현:
   $$= ∑∑ log₂(p(y)/p(x,y)) p(X=x,Y=y)$$
   - 기댓값을 명시적인 합으로 펼친 형태입니다.

### 과제 부분의 관계식
이미지 하단의 과제에서는 다음 관계식을 증명하도록 요청하고 있습니다:
$$H_{p_{x,y}}(Y|X=x) = H(X,Y) - H(X)$$

## 관계식 증명

이 관계식은 엔트로피의 연쇄 법칙(Chain Rule)으로 알려져 있으며, 다음과 같이 증명할 수 있습니다:

1. 조건부 엔트로피 H(Y|X)의 정의에서 시작:
   $$H(Y|X) = ∑_{x} p(X=x) H(Y|X=x) = ∑_{x} p(x) ∑_{y} p(y|x) log₂(1/p(y|x))$$

2. 결합 엔트로피 H(X,Y)의 정의:
   $$H(X,Y) = ∑_{x,y} p(x,y) log₂(1/p(x,y))$$

3. 결합확률의 분해:
   $$p(x,y) = p(x) × p(y|x)$$

4. 이를 H(X,Y)에 대입하여 전개:
   $$H(X,Y) = ∑_{x,y} p(x,y) log₂(1/(p(x)·p(y|x)))$$
   $$= ∑_{x,y} p(x,y) [log₂(1/p(x)) + log₂(1/p(y|x))]$$
   $$= ∑_{x,y} p(x,y) log₂(1/p(x)) + ∑_{x,y} p(x,y) log₂(1/p(y|x))$$

5. 첫 항은 H(X)로 정리:
   $$∑_{x,y} p(x,y) log₂(1/p(x)) = ∑_{x} p(x) log₂(1/p(x)) = H(X)$$

6. 두 번째 항은 H(Y|X)로 정리:
   $$∑_{x,y} p(x,y) log₂(1/p(y|x)) = ∑_{x} p(x) ∑_{y} p(y|x) log₂(1/p(y|x)) = H(Y|X)$$

7. 따라서:
   $$H(X,Y) = H(X) + H(Y|X)$$

8. 이로부터:
   $$H(Y|X) = H(X,Y) - H(X)$$

이 관계식은 "X에 대한 정보와 X가 주어졌을 때 Y에 대한 추가 정보의 합이 X와 Y에 대한 전체 정보와 같다"는 정보이론의 중요한 원리를 표현합니다.